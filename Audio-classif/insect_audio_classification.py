# -*- coding: utf-8 -*-
"""insect_audio_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UJ7eV_FOuYUenad_x5NxnnSfKVAANOdp
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import librosa
import librosa.display
from tqdm import tqdm
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
import tensorflow.keras.models as models
import tensorflow.keras.layers as layers
import IPython.display as ipd
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline
# %load_ext tensorboard
import os
import csv 
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn import svm

def preprocessing_data(path_train, path_test, path_validate):
  """
  Rewriting the directories names and file names, so they're conneceted to train, validate and test.
  """
  path_train = path_train
  path_test = path_test
  path_validate = path_validate

  for file in os.listdir(path_train):
    source = path_train+file
    new_file = file[0:file.index("_")] + ".wav"
    new_source = path_train+new_file
    os.rename(source, new_source)

  for file in os.listdir(path_test):
    source = path_test+file
    new_file = file[0:file.index("_")] + ".wav"
    new_source = path_test+new_file
    os.rename(source, new_source)
    
  for file in os.listdir(path_validate):
    source = path_validate+file
    new_file = file[0:file.index("_")] + ".wav"
    new_source = path_validate+new_file
    os.rename(source, new_source)
  
  arg_train = os.listdir(path_train)
  arg_test = os.listdir(path_test)
  arg_validate = os.listdir(path_validate)

  return arg_train, arg_test, arg_validate

def featurextraction(arg_train, arg_test, arg_validate):
  """
  creates important features of aall the files, and puts them in a .csv
  """
  path_train = 'ManualTrainClean/'
  path_test = 'ManualTestClean/'
  path_validate = 'ManualValidationClean/'

  header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'
  for i in range(1, 21):
      header += f' mfcc{i}'
  header += ' label'
  header = header.split()

  file = open('dataset.csv', 'w', newline='')
  with file:
      writer = csv.writer(file)
      writer.writerow(header)
  insects = 'ManualTrainClean ManualValidationClean ManualTestClean'.split()
  for g in insects:
      for filename in os.listdir(f'/content/drive/MyDrive/SE project/Data/Audio Data/Insect Audio_Classification/{g}'):
          songname = f'/content/drive/MyDrive/SE project/Data/Audio Data/Insect Audio_Classification/{g}/{filename}'
          y, sr = librosa.load(songname, mono=True, duration=30)
          rmse = librosa.feature.rms(y=y)[0]
          chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)
          spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)
          spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
          rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
          zcr = librosa.feature.zero_crossing_rate(y)
          mfcc = librosa.feature.mfcc(y=y, sr=sr)
          to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    
          for e in mfcc:
              to_append += f' {np.mean(e)}'
          to_append += f' {filename[0:14]}'
          file = open('dataset.csv', 'a', newline='')
          with file:
              writer = csv.writer(file)
              writer.writerow(to_append.split())

def loading_dataframe(dataframe_csv):
  df = pd.read_csv(dataframe_csv)
  return df

def set_label(row):
  """
  Setting labels for species. To binarize them later.
  """
  if (row["label"] == "Chorthippusbig") or (row["label"] == "Chorthippusbru") or (row["label"] == "Grylluscampest") or (row["label"] == "Nemobiussylves") or (row["label"] == "Oecanthuspellu") or (row["label"] == "Pholidopteragr") or  (row["label"] == "Pseudochorthip") or (row["label"] == "Roeselianaroes") or (row["label"] == "Tettigoniaviri"):
      return "Orthoptera"
  else:
      return "Cicadidae"
  
  return df

def preprocessing_data(df):
  """
  Binarization labels. By dropping unneccesary columns and splitting the data in train and test sets.v
  """
  df = df.assign(order=df.apply(set_label, axis=1)) 
  df.drop(columns=['label'], axis = 1, inplace = True)
  data = data.drop(['filename'],axis=1)
  genre_list = data.iloc[:, -1]
  encoder = LabelEncoder()
  y = encoder.fit_transform(genre_list)
  scaler = StandardScaler()
  X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

  return X_train, X_test, y_train, y_test

def svm(X_train, X_test, y_train, y_test):
  """
  Training and testing SVM. Outputting accuracy.
  """
  clf = svm.SVC()
  clf.fit(X_train, y_train)
  predictions = clf.predict(X_test)
  score = clf.score(X_test, y_test)

  return score

path_train = '/ManualTrain'
path_test = '/ManualTest'
path_validate = '/ManualValidation'
arg_train, arg_test, arg_validate = preprocessing_data(path_train, path_test, path_validate)
featurextraction(arg_train, arg_test, arg_validate)
dataframe_csv = '/klasyfikacja - Arkusz1.csv'
loading_dataframe(dataframe_csv)

df = set_label(row)
X_train, X_test, y_train, y_test = preprocessing_data(df)
svm(X_train, X_test, y_train, y_test)

def create_dataset_features(insects= 'ManualTrain ManualValidation ManualTest'):
    """
    Preprocessing and feature extraction for new predictions.
    """
    header = 'filename chroma_stft rmse spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'
    for i in range(1, 21):
        header += f' mfcc{i}'
    header += ' label'
    header = header.split()

    file = open('dataset2.csv', 'w', newline='')
    with file:
        writer = csv.writer(file)
        writer.writerow(header)
    insects = insects.split()
    for g in insects:
        for filename in os.listdir(f'D:/DataScience/AUDIO/{g}'):
            songname = f'D:/DataScience/AUDIO/{g}/{filename}'
            y, sr = librosa.load(songname, mono=True, duration=30)
            rmse = librosa.feature.rms(y=y)[0]
            chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)
            spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)
            spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
            rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
            zcr = librosa.feature.zero_crossing_rate(y)
            mfcc = librosa.feature.mfcc(y=y, sr=sr)
            to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    
            for e in mfcc:
                to_append += f' {np.mean(e)}'
            to_append += f' {filename[0:14]}'
            file = open('dataset2.csv', 'a', newline='')
            with file:
                writer = csv.writer(file)
                writer.writerow(to_append.split())

def predict(df_test: pd.DataFrame, model = svm, scaler = scaler):
    """
    Predictions.
    """
    data = df_test.copy()
    data.head()# Dropping unneccesary columns
    data = data.drop(columns=['filename'],axis=1)
    X = scaler.transform(np.array(data.iloc[:, :-1], dtype = float))#Dividing data into training and Testing set
    
    return model.predict(X)

predict(df_test, svm, scaler)